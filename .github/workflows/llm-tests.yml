name: LLM Integration Tests

on:
  workflow_dispatch:  # Manual trigger
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  pull_request:
    paths:
      - 'bpmn_agent/core/llm_client.py'
      - 'bpmn_agent/stages/**/*.py'
      - 'tests/test_*llm*.py'

jobs:
  test-ollama:
    runs-on: ubuntu-latest
    services:
      ollama:
        image: ollama/ollama
        ports:
          - 11434:11434
        options: >-
          --health-cmd "curl -f http://localhost:11434/api/tags || exit 1"
          --health-interval 30s
          --health-timeout 10s
          --health-retries 5
          --health-start-period 60s
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Wait for Ollama service
      run: |
        timeout 120 bash -c 'until curl -f http://localhost:11434/api/tags; do sleep 10; done'
    
    - name: Pull model
      run: |
        docker exec ${{ job.services.ollama.id }} ollama pull mistral
        docker exec ${{ job.services.ollama.id }} ollama pull llama2
    
    - name: Run Ollama integration tests
      env:
        LLM_PROVIDER: ollama
        LLM_BASE_URL: http://localhost:11434
        LLM_MODEL: mistral
      run: |
        pytest -m "llm" --tb=short -v --timeout=600
      timeout-minutes: 20

  test-openai:
    runs-on: ubuntu-latest
    if: secrets.OPENAI_API_KEY != ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Run OpenAI integration tests
      env:
        LLM_PROVIDER: openai
        LLM_BASE_URL: https://api.openai.com/v1
        LLM_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        LLM_MODEL: gpt-3.5-turbo
      run: |
        pytest -m "llm" --tb=short -v --timeout=300
      timeout-minutes: 15

  # Comprehensive E2E test with both Ollama and OpenAI
  e2e-integration:
    runs-on: ubuntu-latest
    needs: [test-ollama, test-openai]
    if: always() && (needs.test-ollama.result == 'success' || needs.test-openai.result == 'success')
    
    services:
      ollama:
        image: ollama/ollama
        ports:
          - 11434:11434
        options: >-
          --health-cmd "curl -f http://localhost:11434/api/tags || exit 1"
          --health-interval 30s
          --health-timeout 10s
          --health-retries 5
          --health-start-period 60s
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install -e ".[ollama]"
    
    - name: Wait for Ollama service
      run: |
        timeout 120 bash -c 'until curl -f http://localhost:11434/api/tags; do sleep 10; done'
    
    - name: Pull model
      run: |
        docker exec ${{ job.services.ollama.id }} ollama pull mistral
    
    - name: Run comprehensive E2E tests
      env:
        LLM_PROVIDER: ollama
        LLM_BASE_URL: http://localhost:11434
        LLM_MODEL: mistral
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL }}
      run: |
        pytest -m "e2e or integration" -v --tb=short --timeout=600 --cov=bpmn_agent --cov-append
      timeout-minutes: 30